{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32c6630",
   "metadata": {},
   "source": [
    "# DAMO-630-29 Assignment 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4fe4dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn pandas numpy matplotlib seaborn sdv \n",
    "#attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b115ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer, CTGANSynthesizer\n",
    "from sdmetrics.reports.single_table import QualityReport, DiagnosticReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b218eaa7",
   "metadata": {},
   "source": [
    "## BUSINESS CHALLENGE #01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280c7a0",
   "metadata": {},
   "source": [
    "# TASK I - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf166c0e",
   "metadata": {},
   "source": [
    "The EDA offers an initial overview of the dataset by inspecting its structure, detecting missing values or outliers, and applying descriptive statistics with visualizations. These insights provide the foundation for subsequent synthetic data generation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62074baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C — Recreate `df_for_clustering` & `scaler_model` if missing (idempotent)\n",
    "from pyspark.sql import DataFrame as _SparkDF\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "df = globals().get('df', None)\n",
    "if globals().get('df_for_clustering', None) is not None:\n",
    "    print('✅ `df_for_clustering` already present — no action taken.')\n",
    "else:\n",
    "    if df is None:\n",
    "        print('⚠️ `df` not found. Please run BC2.I cell or run Cell B to reload the dataset first.')\n",
    "    else:\n",
    "        try:\n",
    "            import pandas as _pd\n",
    "        except Exception:\n",
    "            _pd = None\n",
    "\n",
    "        if isinstance(df, _SparkDF):\n",
    "            candidates = ['trip_distance','fare_amount','tip_amount','tolls_amount','total_amount']\n",
    "            present = globals().get('clustering_numeric_cols', None)\n",
    "            if not present:\n",
    "                present = [c for c in candidates if c in df.columns]\n",
    "                globals()['clustering_numeric_cols'] = present\n",
    "            if not present:\n",
    "                raise RuntimeError('No numeric columns available for clustering.')\n",
    "\n",
    "            # Build cleaned DataFrame similar to Step 2\n",
    "            dfk = df.select(*[F.col(c).cast('double').alias(c) for c in present])\n",
    "            dfk = dfk.na.drop(subset=present)\n",
    "            for c in present:\n",
    "                if c in ('trip_distance', 'fare_amount', 'total_amount'):\n",
    "                    dfk = dfk.filter(F.col(c) > 0)\n",
    "                else:\n",
    "                    dfk = dfk.filter(F.col(c) >= 0)\n",
    "\n",
    "            # Optional: trim extreme outliers (same logic as Step 2)\n",
    "            trim_bounds = {}\n",
    "            for c in present:\n",
    "                try:\n",
    "                    p999 = dfk.approxQuantile(c, [0.999], 0.01)[0]\n",
    "                    if p999 is not None:\n",
    "                        trim_bounds[c] = p999\n",
    "                        dfk = dfk.filter(F.col(c) <= F.lit(p999))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            print('✅ Cleaning done. Rows after cleaning:', dfk.count())\n",
    "            if trim_bounds:\n",
    "                print('Applied p99.9 trims:', trim_bounds)\n",
    "\n",
    "            # Assemble & scale\n",
    "            assembler = VectorAssembler(inputCols=present, outputCol='features')\n",
    "            vec = assembler.transform(dfk)\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='features_scaled', withMean=True, withStd=True)\n",
    "            scaler_model = scaler.fit(vec)\n",
    "            vecs = scaler_model.transform(vec).select(*present, 'features_scaled').cache()\n",
    "\n",
    "            print('✅ Vectorization & scaling complete. Sample:')\n",
    "            vecs.show(5, truncate=False)\n",
    "\n",
    "            globals()['df_for_clustering'] = vecs\n",
    "            globals()['scaler_model'] = scaler_model\n",
    "            print('✅ Stored `df_for_clustering` and `scaler_model` in globals.')\n",
    "\n",
    "        else:\n",
    "            # pandas path: try to convert to Spark if a Spark session exists, otherwise notify user\n",
    "            if _pd is not None and isinstance(df, _pd.DataFrame):\n",
    "                spark = globals().get('spark', None)\n",
    "                if spark is None:\n",
    "                    print('⚠️ `df` is a pandas DataFrame and no Spark session is available.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print('⚠️ Unsupported `df` type. Expected Spark or pandas DataFrame.')            else:                        print('❌ Conversion to Spark failed or subsequent processing failed:', e_conv)                    except Exception as e_conv:                        print('✅ Created `df_for_clustering` after converting pandas to Spark.')                        globals()['scaler_model'] = scaler_model                        globals()['df_for_clustering'] = vecs                        vecs = scaler_model.transform(vec).select(*present, 'features_scaled').cache()                        scaler_model = scaler.fit(vec)                        scaler = StandardScaler(inputCol='features', outputCol='features_scaled', withMean=True, withStd=True)                        vec = assembler.transform(dfk)                        assembler = VectorAssembler(inputCols=present, outputCol='features')                                dfk = dfk.filter(F.col(c) >= 0)                            else:                                dfk = dfk.filter(F.col(c) > 0)                            if c in ('trip_distance', 'fare_amount', 'total_amount'):                        for c in present:                        dfk = dfk.na.drop(subset=present)                        dfk = sdf.select(*[F.col(c).cast('double').alias(c) for c in present])                            raise RuntimeError('No numeric columns available for clustering after conversion.')                        if not present:                            globals()['clustering_numeric_cols'] = present                            present = [c for c in candidates if c in sdf.columns]                        if not present:                        present = globals().get('clustering_numeric_cols', None)                        candidates = ['trip_distance','fare_amount','tip_amount','tolls_amount','total_amount']                        # Proceed with the same Spark path after conversion                        print('✅ Conversion to Spark DataFrame succeeded — re-running Spark path.')                        globals()['df'] = sdf                        sdf = spark.createDataFrame(df)                    try:                    print('ℹ️ Converting pandas DataFrame to Spark DataFrame then proceeding (this may be slow).')                else:KMeans steps require Spark (pyspark.ml). Either start Spark or convert the workflow to scikit-learn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d05851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
