{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32c6630",
   "metadata": {},
   "source": [
    "# DAMO-630-29 Assignment 01\n",
    "prublic repository:  https://github.com/prumucena1979/UNFTERM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84720830",
   "metadata": {},
   "source": [
    "#Business Challenge 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer, CTGANSynthesizer\n",
    "from sdmetrics.reports.single_table import QualityReport, DiagnosticReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d01d98",
   "metadata": {},
   "source": [
    "# TASK I - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e6df2",
   "metadata": {},
   "source": [
    "The EDA offers an initial overview of the dataset by inspecting its structure, detecting missing values or outliers, \n",
    "and applying descriptive statistics with visualizations. These insights provide the foundation for subsequent synthetic data generation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. load dataset\n",
    "df = pd.read_csv(\"Datasets\\HealthInsurance.csv\")  # adjust file name as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. Shape\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3. Preview\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9696c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4. Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5. Descriptive statistics\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6. Missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 1.7. Distribution plots (example numeric columns)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns[:3]\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9cd19",
   "metadata": {},
   "source": [
    "# Task II â€” Baseline Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Random noise baseline\n",
    "synthetic_baseline = pd.DataFrame(\n",
    "    np.random.randn(df.shape[0], df.shape[1]),\n",
    "    columns=df.columns\n",
    ")\n",
    "display(synthetic_baseline.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae05ae0",
   "metadata": {},
   "source": [
    "# Task III â€” Advanced Synthetic Data (SDV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Infer table metadata (types, constraints, relations)\n",
    "metadata = Metadata.detect_from_dataframe(data=df, table_name=\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Synthetic Data Generation â€” GaussianCopula (for SDV â‰¤ 1.3.x)\n",
    "\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata  # fallback import for older SDV versions\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Create metadata from the dataframe ---\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df)\n",
    "\n",
    "# âœ… Save metadata for reproducibility\n",
    "metadata.save_to_json(\"metadata.json\")\n",
    "print(\"âœ… Metadata saved as metadata.json\")\n",
    "\n",
    "# --- Step 2: Fit the GaussianCopula model ---\n",
    "gc = GaussianCopulaSynthesizer(metadata)\n",
    "gc.fit(df)\n",
    "print(\"âœ… GaussianCopula model trained successfully!\")\n",
    "\n",
    "# --- Step 3: Generate synthetic data ---\n",
    "synthetic_gc = gc.sample(num_rows=len(df))\n",
    "display(synthetic_gc.head())\n",
    "\n",
    "# --- Step 4: Optional â€” Save the trained model itself ---\n",
    "gc.save(\"gaussian_copula_synth.pkl\")\n",
    "print(\"ðŸ’¾ Model saved as gaussian_copula_synth.pkl\")\n",
    "\n",
    "# --- Step 5 (optional) â€” Reload later without retraining ---\n",
    "# gc_loaded = GaussianCopulaSynthesizer.load(\"gaussian_copula_synth.pkl\")\n",
    "# synthetic_gc = gc_loaded.sample(num_rows=len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a345aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 CTGAN\n",
    "#ctgan = CTGANSynthesizer(metadata, epochs=200, batch_size=100, verbose=True) - We will let it leaner just for the video.\n",
    "ctgan = CTGANSynthesizer(metadata, epochs=15, batch_size=15, verbose=True)\n",
    "ctgan.fit(df)\n",
    "synthetic_ctgan = ctgan.sample(num_rows=len(df))\n",
    "display(synthetic_ctgan.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00b303",
   "metadata": {},
   "source": [
    "# Task IV â€” Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a2c8e",
   "metadata": {},
   "source": [
    "Convert metadata for sdmetrics (single table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metadata for sdmetrics (single table)\n",
    "_meta_dict = metadata.to_dict()\n",
    "if \"tables\" in _meta_dict:\n",
    "    _table_name = next(iter(_meta_dict[\"tables\"].keys()))\n",
    "    single_table_meta = _meta_dict[\"tables\"][_table_name]\n",
    "else:\n",
    "    single_table_meta = _meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Quality and Diagnostics\n",
    "qr_gc = QualityReport(); qr_gc.generate(df, synthetic_gc, single_table_meta)\n",
    "qr_ct = QualityReport(); qr_ct.generate(df, synthetic_ctgan, single_table_meta)\n",
    "\n",
    "print(\"Quality â€” GC:\", qr_gc.get_score())\n",
    "print(\"Quality â€” CTGAN:\", qr_ct.get_score())\n",
    "\n",
    "dr_gc = DiagnosticReport(); dr_gc.generate(df, synthetic_gc, single_table_meta)\n",
    "dr_ct = DiagnosticReport(); dr_ct.generate(df, synthetic_ctgan, single_table_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea692f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Correlation Preservation\n",
    "def corr_rmse(a, b):\n",
    "    cols = a.select_dtypes(include=np.number).columns.intersection(\n",
    "        b.select_dtypes(include=np.number).columns\n",
    "    )\n",
    "    if len(cols) < 2:\n",
    "        return np.nan\n",
    "    ca, cb = a[cols].corr(), b[cols].corr()\n",
    "    mask = np.triu(np.ones_like(ca, dtype=bool), k=1)\n",
    "    diff = (ca - cb).where(mask)\n",
    "    vals = diff.values[~np.isnan(diff.values)]\n",
    "    return np.sqrt(np.mean(vals**2)) if len(vals) else np.nan\n",
    "\n",
    "print(\"Correlation RMSE â€” GC:\", corr_rmse(df, synthetic_gc))\n",
    "print(\"Correlation RMSE â€” CTGAN:\", corr_rmse(df, synthetic_ctgan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a710681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Utility â€” TSTR (Train on Synthetic, Test on Real)\n",
    "def tstr_classification(real_df, synth_df, target):\n",
    "    Xs, ys = synth_df.drop(columns=[target]), synth_df[target]\n",
    "    Xr, yr = real_df.drop(columns=[target]), real_df[target]\n",
    "    Xs = Xs.select_dtypes(include=np.number).fillna(Xs.median(numeric_only=True))\n",
    "    Xr = Xr.select_dtypes(include=np.number).fillna(Xr.median(numeric_only=True))\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "    clf.fit(Xs, ys)\n",
    "    pred = clf.predict(Xr)\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(yr, pred),\n",
    "        \"f1_macro\": f1_score(yr, pred, average=\"macro\")\n",
    "    }\n",
    "    if len(clf.classes_) == 2:\n",
    "        out[\"roc_auc\"] = roc_auc_score(yr, clf.predict_proba(Xr)[:, 1])\n",
    "    return out\n",
    "\n",
    "def tstr_regression(real_df, synth_df, target):\n",
    "    Xs, ys = synth_df.drop(columns=[target]), synth_df[target]\n",
    "    Xr, yr = real_df.drop(columns=[target]), real_df[target]\n",
    "    Xs = Xs.select_dtypes(include=np.number).fillna(Xs.median(numeric_only=True))\n",
    "    Xr = Xr.select_dtypes(include=np.number).fillna(Xr.median(numeric_only=True))\n",
    "    reg = RandomForestRegressor(n_estimators=400, random_state=42)\n",
    "    reg.fit(Xs, ys)\n",
    "    pred = reg.predict(Xr)\n",
    "    return {\"r2\": r2_score(yr, pred), \"mae\": mean_absolute_error(yr, pred)}\n",
    "\n",
    "# Example (uncomment and set target column)\n",
    "# print(tstr_classification(df, synthetic_gc, \"your_target\"))\n",
    "# print(tstr_classification(df, synthetic_ctgan, \"your_target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Privacy â€” exact duplicates\n",
    "def exact_dup_rate(real_df, synth_df):\n",
    "    r = real_df.astype(str).agg(\"|\".join, axis=1)\n",
    "    s = synth_df.astype(str).agg(\"|\".join, axis=1)\n",
    "    return len(set(r) & set(s)) / max(1, len(s))\n",
    "\n",
    "print(\"Duplication rate â€” GC:\", exact_dup_rate(df, synthetic_gc))\n",
    "print(\"Duplication rate â€” CTGAN:\", exact_dup_rate(df, synthetic_ctgan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a91ebb",
   "metadata": {},
   "source": [
    "## BUSINESS CHALLENGE #02: Mining NYC Taxi Trip Data (DAMO630 Learning Outcomes 3 & 4)\n",
    "- **HDFS URI (RPC):** `hdfs://hadoop-VirtualBox:9000`\n",
    "- **Task I (Data Prep):** load parquet from HDFS and create CSV for MapReduce\n",
    "- **Task II (MapReduce):** total fare per pickup zone (HDFS â†’ Hadoop Streaming)\n",
    "- **Task III (PySpark):** FPGrowth (frequent pattern mining)\n",
    "- **Task IV (PySpark):** K-Means (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer, CTGANSynthesizer\n",
    "from sdmetrics.reports.single_table import QualityReport, DiagnosticReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b218eaa7",
   "metadata": {},
   "source": [
    "## BUSINESS CHALLENGE #02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f6ba6",
   "metadata": {},
   "source": [
    "### Task I â€” Data Preparation and Exploration\n",
    "The cells below configure HDFS/Spark, verify the dataset path, load the parquet file from HDFS, show schema and basic stats, and provide an optional CSV export containing (PULocationID, fare_amount) for Hadoop Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BC2.I - Environment constants and Spark setup (idempotent) ---\n",
    "import os, time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Enforce HDFS user = hadoop BEFORE Spark starts\n",
    "# ------------------------------------------------------------------\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hadoop\"   # critical: ensures permissions ok\n",
    "time.sleep(1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Define Hadoop / HDFS endpoints and paths\n",
    "# ------------------------------------------------------------------\n",
    "NN_HOST = \"hadoop-VirtualBox\"   # or the IP if you prefer (e.g. \"192.168.0.149\")\n",
    "NN_PORT = \"9000\"\n",
    "HDFS_URI = f\"hdfs://{NN_HOST}:{NN_PORT}\"\n",
    "DATA_DIR = \"/data/tlc/trips\"\n",
    "HDFS_PARQUET = f\"{DATA_DIR}/yellow_tripdata_2023-05.parquet\"\n",
    "CSV_BASE_DIR = \"/user/hadoop/taxi\"   # hadoopâ€™s own writable home\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Start SparkSession configured for HDFS\n",
    "# ------------------------------------------------------------------\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"BC2_Env\")\n",
    "         .config(\"spark.hadoop.fs.defaultFS\", HDFS_URI)\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Verify connection and effective user\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Using HDFS URI:\", HDFS_URI)\n",
    "print(\"fs.defaultFS =\", spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\"))\n",
    "ugi = spark._jvm.org.apache.hadoop.security.UserGroupInformation.getCurrentUser().getShortUserName()\n",
    "print(\"HDFS effective user:\", ugi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31895978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS JVM probe: ensure DATA_DIR exists and list its contents\n",
    "from py4j.java_gateway import java_import\n",
    "jvm = sc._jvm\n",
    "java_import(jvm, \"org.apache.hadoop.fs.FileSystem\")\n",
    "java_import(jvm, \"org.apache.hadoop.fs.Path\")\n",
    "fs = jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "p = jvm.org.apache.hadoop.fs.Path(DATA_DIR)\n",
    "created = fs.mkdirs(p)\n",
    "print(\"Created HDFS directory:\" if created else \"HDFS directory already present:\", DATA_DIR)\n",
    "print(\"Listing contents of:\", DATA_DIR)\n",
    "for s in fs.listStatus(p):\n",
    "    print(' -', s.getPath().toString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet load and basic exploration (schema, counts, averages, sample)\n",
    "from pyspark.sql import functions as F\n",
    "if not fs.exists(jvm.org.apache.hadoop.fs.Path(HDFS_PARQUET)):\n",
    "    raise FileNotFoundError(f\"Parquet not found: {HDFS_PARQUET} on {HDFS_URI}\")\n",
    "df = spark.read.parquet(f\"{HDFS_URI}{HDFS_PARQUET}\")\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"Row count:\", df.count())\n",
    "print(\"Basic stats:\")\n",
    "df.selectExpr(\"avg(fare_amount) as avg_fare\", \"avg(trip_distance) as avg_distance\", \"avg(passenger_count) as avg_passengers\").show(1, truncate=False)\n",
    "print(\"Sample 5 rows:\")\n",
    "display(df.select(\"PULocationID\", \"DOLocationID\", \"fare_amount\", \"trip_distance\", \"passenger_count\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a037ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2.I.opt â€” Minimal CSV for Hadoop Streaming (fixed to user=hadoop, no fallbacks)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "HDFS_URI = \"hdfs://hadoop-VirtualBox:9000\"\n",
    "OUT_DIR  = f\"{HDFS_URI}/user/hadoop/taxi/yellow_2023-05_mincsv\"\n",
    "\n",
    "# 1) Sanity check: effective HDFS user must be 'hadoop'\n",
    "ugi = spark._jvm.org.apache.hadoop.security.UserGroupInformation.getCurrentUser().getShortUserName()\n",
    "print(\"HDFS effective user:\", ugi)\n",
    "if ugi != \"hadoop\":\n",
    "    raise RuntimeError(\"Effective HDFS user is not 'hadoop'. Restart kernel, set HADOOP_USER_NAME='hadoop' BEFORE creating SparkSession.\")\n",
    "\n",
    "# 2) Build minimal DF and write a single-shard CSV (no header)\n",
    "min_df = (df\n",
    "    .select(\"PULocationID\", \"fare_amount\")\n",
    "    .where(F.col(\"PULocationID\").isNotNull() & F.col(\"fare_amount\").isNotNull())\n",
    ")\n",
    "\n",
    "(min_df.coalesce(1)\n",
    "      .write.mode(\"overwrite\")\n",
    "      .option(\"header\", \"false\")\n",
    "      .csv(OUT_DIR))\n",
    "\n",
    "print(\"âœ… CSV written to:\", OUT_DIR)\n",
    "\n",
    "# 3) List the output with Hadoop FS API\n",
    "sc, jvm = spark.sparkContext, spark.sparkContext._jvm\n",
    "java_import(jvm, \"org.apache.hadoop.fs.FileSystem\")\n",
    "java_import(jvm, \"org.apache.hadoop.fs.Path\")\n",
    "fs   = jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "pout = jvm.org.apache.hadoop.fs.Path(OUT_DIR)\n",
    "\n",
    "print(\"Contents:\")\n",
    "for s in fs.listStatus(pout):\n",
    "    print(\" -\", s.getPath().toString())\n",
    "\n",
    "# 4) Quick read-back to verify data shape (show a few rows)\n",
    "print(\"\\nSample rows (read-back):\")\n",
    "preview = (spark.read\n",
    "    .option(\"header\", \"false\")\n",
    "    .csv(OUT_DIR)\n",
    "    .toDF(\"PULocationID\",\"fare_amount_raw\")\n",
    "    .select(F.col(\"PULocationID\").cast(\"int\"),\n",
    "            F.col(\"fare_amount_raw\").cast(\"double\").alias(\"fare_amount\"))\n",
    ")\n",
    "preview.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edffd1",
   "metadata": {},
   "source": [
    "### Task II â€” MapReduce: Total Fare by Pickup Zone\n",
    "Below are minimal mapper and reducer definitions suitable for Hadoop Streaming. Save these as `mapper.py` and `reducer.py` on the Hadoop VM and run the streaming job there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a36001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2.II.mapper â€” create mapper.py for Hadoop Streaming\n",
    "\n",
    "mapper = r\"\"\"#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split(',')\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    pul, fare = parts[0].strip(), parts[1].strip()\n",
    "    try:\n",
    "        f = float(fare)\n",
    "    except:\n",
    "        continue\n",
    "    print(f\"{pul}\\t{f}\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"mapper.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(mapper)\n",
    "\n",
    "print(\"âœ… mapper.py created successfully.\\nPreview:\\n\")\n",
    "print(mapper[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2.II.reducer â€” create reducer.py for Hadoop Streaming\n",
    "\n",
    "reducer = r\"\"\"#!/usr/bin/env python3\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "acc = defaultdict(float)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    try:\n",
    "        key, val = line.split('\\t')\n",
    "        acc[key] += float(val)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for k, v in acc.items():\n",
    "    print(f\"{k}\\t{v}\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"reducer.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reducer)\n",
    "\n",
    "print(\"âœ… reducer.py created successfully.\\nPreview:\\n\")\n",
    "print(reducer[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f4de9",
   "metadata": {},
   "source": [
    "Example Hadoop Streaming command (run on the Hadoop VM):\n",
    "```\n",
    "hdfs dfs -cat /user/hadoop/taxi/yellow_2023-05_mincsv/part-* | \\\n",
    "  python3 mapper.py | sort | python3 reducer.py > total_fare_by_pu.txt\n",
    "```\n",
    "After the job, inspect `total_fare_by_pu.txt` for total fares per pickup zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0f6ce",
   "metadata": {},
   "source": [
    "### Task III â€” Frequent Pattern Mining (FPGrowth)\n",
    "Create simple baskets from pickup and dropoff location pairs and run FPGrowth to find frequent location pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7593ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2.III â€” Frequent Pair Analysis (Simplified alternative to FPGrowth)\n",
    "# Computes support, confidence, and lift directly for PUâ†’DO pairs\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Base: trips dataframe already loaded as `df`\n",
    "print(\"Dataset rows:\", df.count())\n",
    "\n",
    "# 1) Prepare clean PUâ€“DO pairs\n",
    "pairs = (df\n",
    "    .select(\"PULocationID\",\"DOLocationID\")\n",
    "    .where(F.col(\"PULocationID\").isNotNull() & F.col(\"DOLocationID\").isNotNull())\n",
    "    .select(F.col(\"PULocationID\").cast(\"string\").alias(\"A\"),\n",
    "            F.col(\"DOLocationID\").cast(\"string\").alias(\"B\"))\n",
    ")\n",
    "total_baskets = pairs.count()\n",
    "print(f\"Total valid trips (baskets): {total_baskets:,}\")\n",
    "\n",
    "# 2) Count individual item frequencies (for confidence and lift)\n",
    "item_counts = (pairs\n",
    "    .select(F.col(\"A\").alias(\"id\")).unionByName(pairs.select(F.col(\"B\").alias(\"id\")))\n",
    "    .groupBy(\"id\").count()\n",
    "    .withColumnRenamed(\"count\",\"item_count\")\n",
    ")\n",
    "\n",
    "# 3) Count co-occurrences of Aâ†’B pairs\n",
    "pair_counts = pairs.groupBy(\"A\",\"B\").count().withColumnRenamed(\"count\",\"pair_count\")\n",
    "\n",
    "# 4) Join and compute association metrics\n",
    "stats = (pair_counts\n",
    "    .join(item_counts.withColumnRenamed(\"id\",\"A\"), on=\"A\", how=\"left\")\n",
    "    .join(item_counts.withColumnRenamed(\"id\",\"B\")\n",
    "                    .withColumnRenamed(\"item_count\",\"B_count\"), on=\"B\", how=\"left\")\n",
    "    .withColumn(\"support\",   F.col(\"pair_count\") / F.lit(total_baskets))\n",
    "    .withColumn(\"confidence\",F.col(\"pair_count\") / F.col(\"item_count\"))\n",
    "    .withColumn(\"lift\",      F.col(\"confidence\") / (F.col(\"B_count\") / F.lit(total_baskets)))\n",
    ")\n",
    "\n",
    "# 5) Show strongest directional relations PUâ†’DO\n",
    "top_rules = (stats\n",
    "    .where(F.col(\"support\") >= 0.005)   # >= 0.5% of all trips\n",
    "    .orderBy(F.desc(\"confidence\"), F.desc(\"support\"))\n",
    ")\n",
    "\n",
    "print(\"\\nTop PUâ†’DO pairs by confidence (supportâ‰¥0.5%):\")\n",
    "top_rules.select(\"A\",\"B\",\"pair_count\",\"support\",\"confidence\",\"lift\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2cd1d",
   "metadata": {},
   "source": [
    "### Task IV â€” K-Means Clustering\n",
    "Use numerical features (fare_amount, trip_distance, passenger_count) to cluster trips and inspect cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2.IV â€” KMeans on numeric features (robust + interpretable)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import DenseVector, Vectors\n",
    "\n",
    "# 0) Select & clean numeric columns\n",
    "NUM_COLS = [\"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
    "\n",
    "df_num = (\n",
    "    df.select(*[F.col(c).cast(\"double\").alias(c) for c in NUM_COLS])\n",
    "      .where(F.col(\"fare_amount\").isNotNull() & F.col(\"trip_distance\").isNotNull() & F.col(\"passenger_count\").isNotNull())\n",
    ")\n",
    "\n",
    "# (Optional but helpful) clip extreme outliers using approx quantiles\n",
    "# keeps computation stable for KMeans on a laptop\n",
    "bounds = {}\n",
    "for c in NUM_COLS:\n",
    "    q1, q99 = df_num.approxQuantile(c, [0.01, 0.99], 0.01)\n",
    "    bounds[c] = (q1, q99)\n",
    "\n",
    "df_clipped = df_num\n",
    "for c in NUM_COLS:\n",
    "    lo, hi = bounds[c]\n",
    "    df_clipped = df_clipped.withColumn(c, F.when(F.col(c) < lo, lo).when(F.col(c) > hi, hi).otherwise(F.col(c)))\n",
    "\n",
    "# (Optional) sample for speed if the dataset is very large\n",
    "n_rows = df_clipped.count()\n",
    "if n_rows > 1_000_000:\n",
    "    frac = 1_000_000 / float(n_rows)\n",
    "    print(f\"Sampling to ~1,000,000 rows for clustering (fraction={frac:.4f})\")\n",
    "    df_clipped = df_clipped.sample(False, frac, seed=42)\n",
    "\n",
    "# 1) Assemble & scale (mean-centering helps KMeans)\n",
    "assembler = VectorAssembler(inputCols=NUM_COLS, outputCol=\"features_raw\")\n",
    "df_feat = assembler.transform(df_clipped).select(*NUM_COLS, \"features_raw\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_feat)\n",
    "df_scaled = scaler_model.transform(df_feat).select(*NUM_COLS, \"features\")\n",
    "\n",
    "# 2) Fit KMeans\n",
    "k = 4\n",
    "kmeans = (KMeans()\n",
    "          .setK(k)\n",
    "          .setSeed(42)\n",
    "          .setFeaturesCol(\"features\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setInitMode(\"k-means||\")\n",
    "          .setMaxIter(50)\n",
    "          .setTol(1e-4))\n",
    "\n",
    "model_k = kmeans.fit(df_scaled)\n",
    "\n",
    "# 3) Evaluate & report\n",
    "pred_scaled = model_k.transform(df_scaled)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "sil = evaluator.evaluate(pred_scaled)\n",
    "\n",
    "sizes = model_k.summary.clusterSizes\n",
    "centers_scaled = model_k.clusterCenters()\n",
    "\n",
    "# Unscale centers back to original units for interpretability\n",
    "mu: DenseVector = scaler_model.mean  # per-feature means\n",
    "sd: DenseVector = scaler_model.std   # per-feature stds\n",
    "def unscale(center_vec):\n",
    "    # center_orig = center_scaled * sd + mu\n",
    "    return [center_vec[i] * sd[i] + mu[i] for i in range(len(NUM_COLS))]\n",
    "\n",
    "centers_orig = [unscale(c) for c in centers_scaled]\n",
    "\n",
    "print(f\"\\nKMeans k={k}\")\n",
    "print(f\"Silhouette (euclidean): {sil:.4f}\")\n",
    "print(\"Cluster sizes:\", sizes)\n",
    "print(\"\\nCluster centers (original units: fare_amount, trip_distance, passenger_count):\")\n",
    "for i, co in enumerate(centers_orig):\n",
    "    print(f\"  {i}: fare=${co[0]:.2f}, dist={co[1]:.2f} mi, pax={co[2]:.2f}\")\n",
    "\n",
    "# 4) Peek at a few assignments (join original numeric cols)\n",
    "print(\"\\nSample predictions:\")\n",
    "(pred_scaled\n",
    "   .select(*NUM_COLS, \"prediction\")\n",
    "   .limit(10)\n",
    "   .show(truncate=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979d6d8",
   "metadata": {},
   "source": [
    "## Conclusion and Business Insights\n",
    "- Total fare by pickup zone identifies high-revenue origins for targeted offers and dynamic pricing adjustments.\n",
    "- Frequent pickup/dropoff pairs reveal corridors and hotspots useful for rebalancing drivers and scheduling surge pricing windows.\n",
    "- Clustering of trips (by fare, distance, passengers) can help create service personas (short cheap trips vs long high-fare trips) and optimize vehicle allocation.\n",
    "- Next steps: validate results on multiple months, integrate time-of-day features, and operationalize MapReduce outputs into BI dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
